Project: mushroom classification

In this project, we will use a public dataset from kaggle.com, the “Mushroom Classification” dataset, and try to figure out if a mushroom is edible or not. This dataset includes descriptions of samples corresponding to 23 species of mushrooms. Each species is identified as edible or poisonous. When it comes to mushrooms, there is no simple rule for determining the edibility, and we have to be careful.

Since we have labeled data, to tackle this classification problem we can use supervised learning models, such as logistic regression, ridge classifier, decision tree, Naive Bayes, and neural networks. When we give a model information about a new mushroom, it will tell us it is edible for example. That statement would have a certain percentage chance of being correct. We will compare the models according to their accuracies and find out the best performing one. Let’s upload our dataset first. You can find the dataset on our platform.

We start with the preprocessing of the dataset. You may consider preprocessing as all the steps we take to prepare the data to be used by ML algorithms. Depending on the project it may include converting strings to integers, converting images to grayscale, resizing images or anything like that. After that we’ll move on to the training. As always, we’ll start with importing the required libraries. The dataset includes data from 8124 mushrooms. Each of these mushroom samples have 22 features and they are categorized as edible or poisonous. Let’s read the .csv file using Pandas read_csv() method. We can take a look at the dataset using data.head() function.

As we can see, there are different features, such as the shape or the color of the cap that we can use to create models. Also, all cells have string type values in them. But this is a problem we will deal with later on. Now, to understand the dataset better, we can use some visualization techniques. For example, by creating a bar graph, we can compare the different classes. We’ll start with finding the number of samples per class. The value_counts() method of a Pandas DataFrame returns the necessary information. Now we can create bars for each class and display the graph.

We can see that the number of edible and poisonous samples are very close to each other. This means our data is balanced. If it wasn’t balanced, there would have been a higher chance of the model being more lean towards the class with more samples. And we would have to equalize the number of samples in each class so that our model generalizes better.

Great, we have a better understanding of our data. Now we’ll divide it into features and corresponding labels. In our case, we’ll only use the columns that have an effect on the mushroom being poisonous or not. These are “cap-shape”, “cap-color”, “ring-number” and “ring-type”. Since we won’t be using all columns, we can’t just use .drop() and remove “class” from the dataset. Instead, we’ll use .loc() and give the features we’ll use as a list. As we mentioned earlier, the values are in string format. We need to convert them to integer values to be able to perform mathematical operations with them. We’ll use label encoding for this. Since the X-data has multiple columns, we’ll do this in a for loop so that we can update all columns. For the y data, we can use the encoder directly. Let’s print both X and y to see the final data. Now, we can split our data into training and test datasets. train_test_split from sklearn comes to our help.

Perfect! Our data is ready to be used! Let’s move on to training our models. In machine learning, we can’t say for sure which model will perform the best for our project. That is why we always should try out different models and compare them. And of course, we should also consider trying out different parameters to get the best out of each. For example, using more nodes and more layers on a neural network might lead to a more accurate model but it also may lead to poorer performance. We’ll have to figure it out by trial and error. We use the models we have imported already. Then, we train all models with the X_train and y_train dataset we created. Using the X_test set we make predictions with each model and save results to corresponding variables. Instead of calculating precision, recall, f-1 score and accuracy separately, we can use the classification_report function of sklearn to compare the performances. Let’s create reports for each model. And finally, we print and compare the results of all models. To increase readability, we’ll add headings for each model using simple print lines.

With 91% accuracy, we can see that the Decision Tree algorithm performed best on all types of scores. Looking at the accuracies, we can see that the three models, linear regression, ridge regression and naive bayes performed the worst. If the Decision tree algorithm performed the best, maybe we can take things one step further and try the Random Forest algorithm to see if it works even better.
